# -*- coding: utf-8 -*-
"""bhw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sBYVv8AEWOHSw5iskhmisNZ4NNH0ZqVF
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

import warnings

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm.notebook import tqdm

from dataset import TranslationDataset
from model import Seq2SeqTransformer
from train import train

warnings.filterwarnings('ignore', category=UserWarning)

SRC_LANGUAGE = 'de'
TRG_LANGUAGE = 'en'
BATCH_SIZE = 64

train_dataset = TranslationDataset('data/train.de-en.de', 'data/train.de-en.en', SRC_LANGUAGE, TRG_LANGUAGE)
val_dataset = TranslationDataset('data/val.de-en.de', 'data/val.de-en.en', SRC_LANGUAGE, TRG_LANGUAGE)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                          collate_fn=train_dataset.build_collate_fn())
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=val_dataset.build_collate_fn())

SRC_VOCAB_SIZE, TRG_VOCAB_SIZE = train_dataset.vocab_sizes

print(f"Vocabs are built with lengths:\n"
      f"Train - {SRC_VOCAB_SIZE}\n"
      f"Val - {TRG_VOCAB_SIZE}")

EMB_SIZE = 512
NHEAD = 8
FFN_HID_DIM = 1024
NUM_ENCODER_LAYERS = 4
NUM_DECODER_LAYERS = 4
DROPOUT = 0.1
MAX_LEN = 128

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,
                           NHEAD, SRC_VOCAB_SIZE, TRG_VOCAB_SIZE, FFN_HID_DIM, DROPOUT, train_dataset, MAX_LEN).to(
    device)

for p in model.parameters():
    if p.dim() > 1:
        nn.init.xavier_uniform_(p)

optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)

NUM_EPOCHS = 7

train(model, train_loader, val_loader, optimizer, NUM_EPOCHS, device)

test_dataset = TranslationDataset('data/test1.de-en.de', src_language=SRC_LANGUAGE, is_test=True)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=test_dataset.build_collate_fn())
trg_vocab = train_dataset.vocabs[TRG_LANGUAGE]

cnt = 0
with open('predictions.txt', 'w') as pred:
    for src, trg in tqdm(test_loader, desc='Predicting'):
        translation = model.translate(src, trg_vocab, device)
        pred.write(translation + '\n')
        if cnt < 7:
            print(translation)
        cnt += 1
